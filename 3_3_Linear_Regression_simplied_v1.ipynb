{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.3 Linear Regression_simplied.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP00IJH1+Vbm4w4NEVZJk7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronshi2017/pytorch-Deep-Learning/blob/master/3_3_Linear_Regression_simplied_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hFtMlBtSrju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f13162-3966-4029-db9c-0f121ce3c781"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "!pip install d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "#define the true value of w (weight权重) and b (bias 偏置)并换成张量形式\n",
        "true_w=torch.tensor([2,-3.4]) \n",
        "true_b=4.2\n",
        "features,labels=d2l.synthetic_data(true_w,true_b, 1000) #生成线性回归训练集 refer to following original code for synthetic_data function, features is X, labels is y, they are generated based on givein w and b values\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-0.17.0-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 30 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 51 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 83 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (7.6.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.6.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.0.5)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (57.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.1.0->ipykernel->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (1.0.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l) (4.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.10.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l) (22.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (1.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (4.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (21.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (1.24.3)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmfSxARZqihF"
      },
      "source": [
        "# # Defined in file: ./chapter_linear-networks/linear-regression-scratch.md\n",
        "# [docs]def synthetic_data(w, b, num_examples):\n",
        "#     \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
        "#     X = d2l.normal(0, 1, (num_examples, len(w)))\n",
        "#     y = d2l.matmul(X, w) + b\n",
        "#     y += d2l.normal(0, 0.01, y.shape) #adding noise normal distribution with mean 0, variance 0.01\n",
        "#     return X, d2l.reshape(y, (-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aecqTGJjG1ck"
      },
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):  \n",
        "    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n",
        "    dataset = data.TensorDataset(*data_arrays) #Dataset wrapping tensors.\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "batch_size = 10 # use batch size =10 to do gradient descent algorithm\n",
        "data_iter = load_array((features, labels), batch_size)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc7uWJz5oB60"
      },
      "source": [
        "#https://discuss.pytorch.org/t/make-a-tensordataset-and-dataloader-with-multiple-inputs-parameters/26605/5\n",
        "# Example how to build up Dataloader \n",
        "# nb_samples = 100\n",
        "# features = torch.randn(nb_samples, 10)\n",
        "# labels = torch.empty(nb_samples, dtype=torch.long).random_(10)\n",
        "# adjacency = torch.randn(nb_samples, 5)\n",
        "# laplacian = torch.randn(nb_samples, 7)\n",
        "\n",
        "# dataset = TensorDataset(features, labels, adjacency, laplacian)\n",
        "# loader = DataLoader(\n",
        "#     dataset,\n",
        "#     batch_size=2\n",
        "# )\n",
        "\n",
        "# for batch_idx, (x, y, a, l) in enumerate(loader):\n",
        "#     print(x.shape, y.shape, a.shape, l.shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmmS4Fesox36",
        "outputId": "3a6d0929-48c9-4cfc-b6eb-47af8f4e03f6"
      },
      "source": [
        "next(iter(data_iter)) # demo how to use next(iter) to iterate the data "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 0.7477,  0.4514],\n",
              "         [-0.8593,  0.3605],\n",
              "         [ 1.5132,  0.4942],\n",
              "         [-0.0871, -2.2118],\n",
              "         [ 1.6509,  0.0791],\n",
              "         [-1.7786,  0.9295],\n",
              "         [ 0.7407,  0.6482],\n",
              "         [ 0.0981,  0.1206],\n",
              "         [ 1.5421,  0.1012],\n",
              "         [ 0.6602,  0.2343]]), tensor([[ 4.1736],\n",
              "         [ 1.2414],\n",
              "         [ 5.5460],\n",
              "         [11.5527],\n",
              "         [ 7.1976],\n",
              "         [-2.5218],\n",
              "         [ 3.4618],\n",
              "         [ 3.9671],\n",
              "         [ 6.9314],\n",
              "         [ 4.7202]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YchtCJCpZI-G",
        "outputId": "65f48a4d-d628-4c5f-9d7b-14701e1b6097"
      },
      "source": [
        "next(iter(data_iter))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-2.3749e-01,  1.9942e+00],\n",
              "         [-5.3149e-01, -7.0952e-01],\n",
              "         [-1.4470e-03, -1.2178e+00],\n",
              "         [-6.6078e-01,  2.1827e-01],\n",
              "         [ 6.2129e-01, -6.0856e-01],\n",
              "         [ 1.3483e+00,  8.3115e-01],\n",
              "         [ 2.2468e+00, -6.0395e-01],\n",
              "         [ 1.2220e+00,  1.9590e-01],\n",
              "         [ 1.4360e-01, -6.4285e-01],\n",
              "         [ 1.9252e+00,  1.6720e+00]]), tensor([[-3.0746],\n",
              "         [ 5.5519],\n",
              "         [ 8.3276],\n",
              "         [ 2.1283],\n",
              "         [ 7.5343],\n",
              "         [ 4.0535],\n",
              "         [10.7319],\n",
              "         [ 5.9621],\n",
              "         [ 6.6655],\n",
              "         [ 2.3577]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stThnfvj4-db"
      },
      "source": [
        "对于标准操作，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。我们首先定义一个模型变量net，它是一个 Sequential 类的实例。 Sequential 类为串联在一起的多个层定义了一个容器。当给定输入数据， Sequential 实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，依此类推。在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉标准的流水线。在 PyTorch 中，全连接层在 Linear 类中定义。值得注意的是，我们将两个参数传递到 nn.Linear 中。第一个指定输入特征形状，即 2，第二个指定输出特征形状，输出特征形状为单个标量，因此为 1。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEmetFji4__-",
        "outputId": "3e9acb9c-2612-471e-d167-074af27e9bc9"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "net = nn.Sequential(nn.Linear(2, 1)) # define all connected layers\n",
        "net[0].weight.data.normal_(0,0.01) #visit the first layer net[0] and reset weight and bias\n",
        "net[0].bias.data.fill_(0) # we initilize w and b with such values. Net denotes the neural network (sequential)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBf0n_AM53u2"
      },
      "source": [
        "loss=nn.MSELoss()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjXXmySJJow"
      },
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2zAPeoXJCfO"
      },
      "source": [
        "https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/\n",
        "Stochastic Gradient Descent (SGD):\n",
        "The word ‘stochastic‘ means a system or a process that is linked with a random probability. Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called “batch” which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Although, using the whole dataset is really useful for getting to the minima in a less noisy and less random manner, but the problem arises when our datasets gets big.\n",
        "Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima is reached. Hence, it becomes computationally very expensive to perform.This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration. SGD equals mini batch gradient descent algorithm with batch size=1.\n",
        "\n",
        "One thing to be noted is that, as SGD is generally noisier than typical Gradient Descent, it usually took a higher number of iterations to reach the minima, because of its randomness in its descent. Even though it requires a higher number of iterations to reach the minima than typical Gradient Descent, it is still computationally much less expensive than typical Gradient Descent. Hence, in most scenarios, SGD is preferred over Batch Gradient Descent for optimizing a learning algorithm.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "\n",
        ">>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        ">>> optimizer.zero_grad()\n",
        "\n",
        ">>> loss_fn(model(input), target).backward()\n",
        "\n",
        ">>> optimizer.step()\n",
        "\n",
        "Momentum [1] or SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
        "https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
        "With Stochastic Gradient Descent we don’t compute the exact derivate of our loss function. Instead, we’re estimating it on a small batch. Which means we’re not always going in the optimal direction, because our derivatives are ‘noisy’. Just like in my graphs above. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations. This is one reason why momentum might work better than classic SGD.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbirR3jCbmoi"
      },
      "source": [
        "# def SGD(f, theta0, alpha, num_iters):\n",
        "#     \"\"\" \n",
        "#        Arguments:\n",
        "#        f -- the function to optimize, it takes a single argument\n",
        "#             and yield two outputs, a cost and the gradient\n",
        "#             with respect to the arguments\n",
        "#        theta0 -- the initial point to start SGD from\n",
        "#        num_iters -- total iterations to run SGD for\n",
        "#        Return:\n",
        "#        theta -- the parameter value after SGD finishes\n",
        "#     \"\"\"\n",
        "#     start_iter = 0\n",
        "#     theta = theta0\n",
        "#     for iter in xrange(start_iter + 1, num_iters + 1):\n",
        "#         _, grad = f(theta)\n",
        "   \n",
        "#         # there is NO dot product ! return theta\n",
        "#         theta = theta - (alpha * grad) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNqSlvm5XxIZ",
        "outputId": "00c5426c-5df3-486f-a38f-d9e243069777"
      },
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X) ,y)\n",
        "        trainer.zero_grad()\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "    l = loss(net(features), labels)\n",
        "    print(f'epoch {epoch + 1}, loss {l:f}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.000214\n",
            "epoch 2, loss 0.000099\n",
            "epoch 3, loss 0.000099\n",
            "epoch 4, loss 0.000098\n",
            "epoch 5, loss 0.000098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJj2pr2CYlnG",
        "outputId": "9ee1e8af-84f9-4914-c469-9d4c64f96c3f"
      },
      "source": [
        "w = net[0].weight.data\n",
        "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
        "b = net[0].bias.data\n",
        "print('b的估计误差：', true_b - b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w的估计误差： tensor([-2.1219e-05, -2.7084e-04])\n",
            "b的估计误差： tensor([0.0005])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG0OhGuZYmcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}